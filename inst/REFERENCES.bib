@article{Koren2009,
author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
doi = {10.1109/MC.2009.263},
file = {:home/david/Recommender-Systems-[Netflix].pdf:pdf},
issn = {0018-9162},
journal = {Computer},
month = {aug},
number = {8},
pages = {30--37},
title = {{Matrix Factorization Techniques for Recommender Systems}},
url = {doi.ieeecomputersociety.org/10.1109/MC.2009.263 http://ieeexplore.ieee.org/document/5197422/},
volume = {42},
year = {2009}
}
@article{Candes2009,
author = {Cand{\`{e}}s, Emmanuel J. and Recht, Benjamin},
doi = {10.1007/s10208-009-9045-5},
file = {:home/david/MatrixCompletion.pdf:pdf},
issn = {1615-3375},
journal = {Foundations of Computational Mathematics},
keywords = {compressed sensing,convex optimization,decou-,duality in optimiza-,low-rank matrices,matrix completion,noncommutative khintchine inequality,nuclear norm minimization,pling,random matrices,tion},
month = {dec},
number = {6},
pages = {717--772},
title = {{Exact Matrix Completion via Convex Optimization}},
url = {http://link.springer.com/10.1007/s10208-009-9045-5},
volume = {9},
year = {2009}
}
@article{Akulenko2016,
abstract = {Batch effects describe non-natural variations of, for example, large-scale genomic data sets. If not corrected by suitable numerical algorithms, batch effects may seriously affect the analysis of these datasets. The novel array platform independent software tool BEclear enables researchers to identify those portions of the data that deviate statistically significant from the remaining data and to replace these portions by typical values reconstructed from neighboring data entries based on latent factor models. In contrast to other comparable methods that often use some sort of global normalization of the data, BEclear avoids changing the apparently unaffected parts of the data. We tested the performance of this approach on DNA methylation data for various tumor data sets taken from The Cancer Genome Atlas and compared the results to those obtained with the existing algorithms ComBat, Surrogate Variable Analysis, RUVm and Functional normalization. BEclear constantly performed at par with or better than these methods. BEclear is available as an R package at the Bioconductor project http://bioconductor.org/packages/release/bioc/html/BEclear.html.},
author = {Akulenko, Ruslan and Merl, Markus and Helms, Volkhard},
doi = {10.1371/journal.pone.0159921},
file = {:home/david/pone.0159921.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pages = {1--17},
pmid = {27559732},
title = {{BEclear: Batch effect detection and adjustment in DNA methylation data}},
volume = {11},
year = {2016}
}

@article{TCGA,
abstract = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed large numbers of human tumors to discover molecular aberrations at the DNA, RNA, protein and epigenetic levels. The resulting rich data provide a major opportunity to develop an integrated picture of commonalities, differences and emergent themes across tumor lineages. The Pan-Cancer initiative compares the first 12 tumor types profiled by TCGA. Analysis of the molecular aberrations and their functional roles across tumor types will teach us how to extend therapies effective in one cancer type to others with a similar genomic profile.},
author = {{Cancer Genome Atlas Research Network} and Weinstein, John N and Collisson, Eric A and Mills, Gordon B and Shaw, Kenna R Mills and Ozenberger, Brad A and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M},
doi = {10.1038/ng.2764},
issn = {1546-1718},
journal = {Nature genetics},
month = {oct},
number = {10},
pages = {1113--20},
pmid = {24071849},
title = {{The Cancer Genome Atlas Pan-Cancer analysis project.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24071849 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3919969},
volume = {45},
year = {2013}
}

@article{BH,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346101},
 abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
 author = {Yoav Benjamini and Yosef Hochberg},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {289--300},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
 volume = {57},
 year = {1995}
}

@article{Dixon1950,
abstract = {Dixon, W. J. (1950). Analysis of extreme values. The Annals of Mathematical Statistics, 21(4), 488-506.},
author = {Dixon, W. J.},
doi = {10.1214/aoms/1177729747},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {dec},
number = {4},
pages = {488--506},
title = {{Analysis of Extreme Values}},
url = {http://projecteuclid.org/euclid.aoms/1177729747},
volume = {21},
year = {1950}
}

@article{Dixon1951,
abstract = {A general weak convergence theory is developed for time-sequential censored rank statistics in the two-sample problem of comparing time to failure between two treatment groups, such as in the case of a clinical trial in which patients enter serially and, after being randomly allocated to one of two treatments, are followed until they fail or withdraw from the study or until the study is terminated. Applications of the theory to time-sequential tests based on these censored rank statistics are also discussed.},
author = {Dixon, W. J.},
doi = {10.1214/aoms/1177729693},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {68--78},
title = {{Ratios Involving Extreme Values}},
url = {http://projecteuclid.org/euclid.aoms/1177729693},
volume = {22},
year = {1951}
}

@article{Rorabacher1991,
abstract = {CrHlcal values at the 95{\%} Confidence level for the two-tailed 0 test, and related tests based upon subrange ratlos, for the statlstkal rejectlon of outlying data have been Interpolated by applying cublc regresslon analysls to the values orlglnally published by Dlxon. Corrections to errors In Dixon's orlglnal tables are also Included. The resultant values are judged to be accurate to wlthln f0.002 and corroborate the fact that correspondlng crltlcal values published In recent slatlstlcal treatlses for analytical chemlsts are erroneous. It Is recom- mended that the newly generated 95{\%} crltlcal values be adopted by analytical chemlsts as the general standard for the rejection of outller values.},
author = {Rorabacher, David B.},
doi = {10.1021/ac00002a010},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = {jan},
number = {2},
pages = {139--146},
title = {{Statistical treatment for rejection of deviant values: critical values of Dixon's "Q" parameter and related subrange ratios at the 95{\%} confidence level}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00002a010},
volume = {63},
year = {1991}
}
